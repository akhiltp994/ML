{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3a84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /home/ibab/.local/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 779 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2021.10.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 851 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.6.5 regex-2021.10.23 tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ec80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13194dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a07d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19bc168",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52e0f91a5d7b74552c55</td>\n",
       "      <td>New Main Picture \\n\\nHow about this for the ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e2c8e370a8e53ba26bae</td>\n",
       "      <td>Think of them like population charts. Just bec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03c807f61149a13c8404</td>\n",
       "      <td>This page seems a little misleading. The reaso...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fc63a1ba3372899db19f</td>\n",
       "      <td>\"\\n\\nActually, accounts are never deleted.   \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0c2bfd9cde8974d9915f</td>\n",
       "      <td>\"\\n\\nYeah yeah, OK.  So did I.  Still, what I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  52e0f91a5d7b74552c55  New Main Picture \\n\\nHow about this for the ma...   \n",
       "1  e2c8e370a8e53ba26bae  Think of them like population charts. Just bec...   \n",
       "2  03c807f61149a13c8404  This page seems a little misleading. The reaso...   \n",
       "3  fc63a1ba3372899db19f     \"\\n\\nActually, accounts are never deleted.   \"   \n",
       "4  0c2bfd9cde8974d9915f  \"\\n\\nYeah yeah, OK.  So did I.  Still, what I ...   \n",
       "\n",
       "   harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate  \n",
       "0      0                0       0            0           0              0  \n",
       "1      0                0       0            0           0              0  \n",
       "2      0                0       0            0           0              0  \n",
       "3      0                0       0            0           0              0  \n",
       "4      0                0       0            0           0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/home/ibab/Desktop/ml_proj/train.csv/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c767a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'text',\n",
       " 'harsh',\n",
       " 'extremely_harsh',\n",
       " 'vulgar',\n",
       " 'threatening',\n",
       " 'disrespect',\n",
       " 'targeted_hate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2043a0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                              52e0f91a5d7b74552c55\n",
      "text               New Main Picture \\n\\nHow about this for the ma...\n",
      "harsh                                                              0\n",
      "extremely_harsh                                                    0\n",
      "vulgar                                                             0\n",
      "threatening                                                        0\n",
      "disrespect                                                         0\n",
      "targeted_hate                                                      0\n",
      "Name: 0, dtype: object\n",
      "id                                              e2c8e370a8e53ba26bae\n",
      "text               Think of them like population charts. Just bec...\n",
      "harsh                                                              0\n",
      "extremely_harsh                                                    0\n",
      "vulgar                                                             0\n",
      "threatening                                                        0\n",
      "disrespect                                                         0\n",
      "targeted_hate                                                      0\n",
      "Name: 1, dtype: object\n",
      "id                                              03c807f61149a13c8404\n",
      "text               This page seems a little misleading. The reaso...\n",
      "harsh                                                              0\n",
      "extremely_harsh                                                    0\n",
      "vulgar                                                             0\n",
      "threatening                                                        0\n",
      "disrespect                                                         0\n",
      "targeted_hate                                                      0\n",
      "Name: 2, dtype: object\n",
      "id                                           fc63a1ba3372899db19f\n",
      "text               \"\\n\\nActually, accounts are never deleted.   \"\n",
      "harsh                                                           0\n",
      "extremely_harsh                                                 0\n",
      "vulgar                                                          0\n",
      "threatening                                                     0\n",
      "disrespect                                                      0\n",
      "targeted_hate                                                   0\n",
      "Name: 3, dtype: object\n",
      "id                                              0c2bfd9cde8974d9915f\n",
      "text               \"\\n\\nYeah yeah, OK.  So did I.  Still, what I ...\n",
      "harsh                                                              0\n",
      "extremely_harsh                                                    0\n",
      "vulgar                                                             0\n",
      "threatening                                                        0\n",
      "disrespect                                                         0\n",
      "targeted_hate                                                      0\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i, j in data.head().iterrows():\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829efd66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5318d4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why you Little...\\nI hope you get banned from Wikipedia someday! And you will! We will meet again you stupid Singaporan fool! But for now, I shall take my break from you, so long FOOL! HAHAHAHAHAHAHAHAHAHAHAHAHA! (Disapears into a puff of smoke).'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[122939,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f11ddd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.4 MB 19 kB/s s eta 0:00:01|████▍                           | 62.1 MB 12.3 MB/s eta 0:00:33\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 89 kB/s eta 0:00:0151\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[K     |████████████████████████████████| 463 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 418 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 89 kB/s eta 0:00:0101\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 634 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/lib/python3/dist-packages (from tensorflow) (3.7.4.1)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 607 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 228 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 78 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.2-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (2.22.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 324 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (45.2.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.1 MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: wrapt, clang, termcolor\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78513 sha256=c8ac093ae630307b50b7c136d60664d0fe1527529aada7682c46c4eb04b89960\n",
      "  Stored in directory: /home/ibab/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=b15daaf5239deba6040fee8b177cec49c2476d5feef883244976c7e7864bc462\n",
      "  Stored in directory: /home/ibab/.cache/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=f83a78c3f360d4517fbbb622c799544e473f81d1b6c9e55689d3f95af0cf1294\n",
      "  Stored in directory: /home/ibab/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built wrapt clang termcolor\n",
      "\u001b[31mERROR: launchpadlib 1.10.13 requires testresources, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: python-javabridge 4.0.3 has requirement numpy>=1.20.1, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: cellprofiler 4.2.1 has requirement h5py==3.2.1, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: cellprofiler 4.2.1 has requirement numpy>=1.20.1, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: cellprofiler-core 4.2.1 has requirement h5py==3.2.1, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wrapt, clang, six, wheel, astunparse, protobuf, numpy, gast, flatbuffers, tensorflow-estimator, keras, grpcio, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, tensorboard-plugin-wit, absl-py, werkzeug, tensorboard, termcolor, google-pasta, opt-einsum, keras-preprocessing, h5py, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.2.1\n",
      "    Uninstalling h5py-3.2.1:\n",
      "      Successfully uninstalled h5py-3.2.1\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 cachetools-4.2.4 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.3.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.1 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 numpy-1.19.5 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 six-1.15.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.7.0 termcolor-1.1.0 werkzeug-2.0.2 wheel-0.37.0 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70e11f",
   "metadata": {},
   "source": [
    "Preprocessing using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddf5e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "421c7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split input into training and testing\n",
    "\n",
    "sentences = data['text']\n",
    "labels = data[['harsh','extremely_harsh','vulgar','threatening','disrespect','targeted_hate']]\n",
    "training_sentences,testing_sentences,training_lab,testing_lab = train_test_split(sentences,labels,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c380afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20dc2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing and padding\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41c4b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim),\n",
    "    tf.keras.layers.GlobalAvgPool1D(),\n",
    "    tf.keras.layers.Dense(24,activation='relu'),\n",
    "    tf.keras.layers.Dense(6,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a65c09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          1600000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 150       \n",
      "=================================================================\n",
      "Total params: 1,600,558\n",
      "Trainable params: 1,600,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c324a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3192/3192 - 27s - loss: 0.1227 - accuracy: 0.9353 - val_loss: 0.0808 - val_accuracy: 0.9936\n",
      "Epoch 2/3\n",
      "3192/3192 - 27s - loss: 0.0668 - accuracy: 0.9944 - val_loss: 0.0665 - val_accuracy: 0.9936\n",
      "Epoch 3/3\n",
      "3192/3192 - 26s - loss: 0.0535 - accuracy: 0.9944 - val_loss: 0.0601 - val_accuracy: 0.9936\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "history = model.fit(training_padded, training_lab, epochs=num_epochs, validation_data=(testing_padded, testing_lab), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "841f748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7111781  0.0189842  0.26677603 0.01026183 0.24219564 0.03380141]\n",
      " [0.1717326  0.00228816 0.04171696 0.0026091  0.04248479 0.00835407]]\n"
     ]
    }
   ],
   "source": [
    "#checking model performance\n",
    "\n",
    "sentence = ['i will kill you, pig', 'i love you']\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96de1ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25f48f649f60423c091b</td>\n",
       "      <td>, 19 May 2006 (UTC)\\nThey debate, they don't v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5c7ac6d7fb400bbadfc7</td>\n",
       "      <td>\"\\n\\nI am completely nonplussed at this \"\"We'v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d00a363d57952496854f</td>\n",
       "      <td>\"\\n\\nUnblock request\\n\\nCategory:User block te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b082c69afa60b378503d</td>\n",
       "      <td>Dave 1185 \\n\\nIf you have a moment, can you he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a585118ed7e1f29b38b</td>\n",
       "      <td>WarningPlease stop adding nonsense to Wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>df7309a82847b06b9078</td>\n",
       "      <td>Deepwater deaths \\n\\nHi, i noticed you were pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15c5221a3e2fcf1e0206</td>\n",
       "      <td>\"\\n\\nActually there is no reason to replace th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a45988e5c44f83b74d6d</td>\n",
       "      <td>\"I've started a background section here, mostl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>69a53955f44fb1e037b6</td>\n",
       "      <td>Welcome to the target list of operation wikira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2ef1e65d78e5bb41954e</td>\n",
       "      <td>IS THIS A JOKE?  \\n\\nThis article is so biased...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text\n",
       "0  25f48f649f60423c091b  , 19 May 2006 (UTC)\\nThey debate, they don't v...\n",
       "1  5c7ac6d7fb400bbadfc7  \"\\n\\nI am completely nonplussed at this \"\"We'v...\n",
       "2  d00a363d57952496854f  \"\\n\\nUnblock request\\n\\nCategory:User block te...\n",
       "3  b082c69afa60b378503d  Dave 1185 \\n\\nIf you have a moment, can you he...\n",
       "4  1a585118ed7e1f29b38b  WarningPlease stop adding nonsense to Wikipedi...\n",
       "5  df7309a82847b06b9078  Deepwater deaths \\n\\nHi, i noticed you were pa...\n",
       "6  15c5221a3e2fcf1e0206  \"\\n\\nActually there is no reason to replace th...\n",
       "7  a45988e5c44f83b74d6d  \"I've started a background section here, mostl...\n",
       "8  69a53955f44fb1e037b6  Welcome to the target list of operation wikira...\n",
       "9  2ef1e65d78e5bb41954e  IS THIS A JOKE?  \\n\\nThis article is so biased..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('/home/ibab/Desktop/ml_proj/test.csv')\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f80cceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        25f48f649f60423c091b\n",
      "1        5c7ac6d7fb400bbadfc7\n",
      "2        d00a363d57952496854f\n",
      "3        b082c69afa60b378503d\n",
      "4        1a585118ed7e1f29b38b\n",
      "                 ...         \n",
      "31910    23e94a16ea98b5bf067c\n",
      "31911    0112c347e9f7d95f292a\n",
      "31912    bdcca21bbd42afa26fad\n",
      "31913    488af1c2ada63cd040f1\n",
      "31914    570cc22ef59108ba8e0a\n",
      "Name: id, Length: 31915, dtype: object [[7.4512959e-03 1.5792251e-04 2.7742684e-03 5.0848722e-04 3.4319460e-03\n",
      "  1.4958978e-03]\n",
      " [7.6726923e-05 1.0054716e-08 5.4798948e-06 4.3956183e-07 5.0695144e-06\n",
      "  3.3491729e-06]\n",
      " [5.6339502e-03 1.3002753e-04 2.1618009e-03 4.6977401e-04 2.6930869e-03\n",
      "  1.3136566e-03]\n",
      " ...\n",
      " [6.8016627e-05 1.1322316e-09 2.6583225e-06 7.3669888e-08 2.0233156e-06\n",
      "  9.2508765e-07]\n",
      " [5.5246055e-03 5.2125069e-06 8.5994601e-04 3.1952924e-05 8.2764030e-04\n",
      "  2.0632148e-04]\n",
      " [1.6673505e-03 1.6517970e-07 1.3419986e-04 2.0138480e-06 1.1505033e-04\n",
      "  2.2573642e-05]]\n"
     ]
    }
   ],
   "source": [
    "sentence = test_data['text']\n",
    "ids = test_data['id']\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "predictions = model.predict(padded)\n",
    "print(ids,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1f2c13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.1109658e-05 7.5445650e-09 5.7825596e-06 3.2464837e-07 5.1511452e-06\n",
      "  2.8292325e-06]]\n"
     ]
    }
   ],
   "source": [
    "sentence = test_data.loc[888,'text']\n",
    "sequences = tokenizer.texts_to_sequences([sentence])\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40e9ceac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nThere is indeed an anchor in the table using the template . Navigation works for me using Internet Explorer 7: clicking on Experientia docet will open List of Latin phrases (C-E) and place the table such that \"\"experientia docet\"\" is positioned at the top of the browser\\'s window. However, it didnt\\'t work here with Firefox 3.0.4; turns out that FF needs exactly agreeing use of case (experientia vs Experientia) when specifying an anchor as target. I have now modified the REDIRECT accordingly.   \"'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0da75563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id     harsh  extremely_harsh    vulgar  \\\n",
      "0      25f48f649f60423c091b  0.007451     1.579225e-04  0.002774   \n",
      "1      5c7ac6d7fb400bbadfc7  0.000077     1.005472e-08  0.000005   \n",
      "2      d00a363d57952496854f  0.005634     1.300275e-04  0.002162   \n",
      "3      b082c69afa60b378503d  0.003272     1.163294e-05  0.000742   \n",
      "4      1a585118ed7e1f29b38b  0.006289     6.731331e-05  0.001965   \n",
      "...                     ...       ...              ...       ...   \n",
      "31910  23e94a16ea98b5bf067c  0.001523     3.673845e-06  0.000304   \n",
      "31911  0112c347e9f7d95f292a  0.218589     2.848059e-03  0.052875   \n",
      "31912  bdcca21bbd42afa26fad  0.000068     1.132232e-09  0.000003   \n",
      "31913  488af1c2ada63cd040f1  0.005525     5.212507e-06  0.000860   \n",
      "31914  570cc22ef59108ba8e0a  0.001667     1.651797e-07  0.000134   \n",
      "\n",
      "        threatening  disrespect  targeted_hate  \n",
      "0      5.084872e-04    0.003432   1.495898e-03  \n",
      "1      4.395618e-07    0.000005   3.349173e-06  \n",
      "2      4.697740e-04    0.002693   1.313657e-03  \n",
      "3      6.744941e-05    0.000835   3.084242e-04  \n",
      "4      2.461374e-04    0.002431   9.005666e-04  \n",
      "...             ...         ...            ...  \n",
      "31910  2.928792e-05    0.000340   1.425445e-04  \n",
      "31911  2.931893e-03    0.052558   9.608924e-03  \n",
      "31912  7.366989e-08    0.000002   9.250876e-07  \n",
      "31913  3.195292e-05    0.000828   2.063215e-04  \n",
      "31914  2.013848e-06    0.000115   2.257364e-05  \n",
      "\n",
      "[31915 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "out_test = pd.DataFrame(columns=['id','harsh','extremely_harsh','vulgar','threatening','disrespect','targeted_hate'])\n",
    "out_test['id'] = ids\n",
    "out_test['harsh'] = predictions[:,0]\n",
    "out_test['extremely_harsh'] = predictions[:,1]\n",
    "out_test['vulgar'] = predictions[:,2]\n",
    "out_test['threatening'] = predictions[:,3]\n",
    "out_test['disrespect'] = predictions[:,4]\n",
    "out_test['targeted_hate'] = predictions[:,5]\n",
    "\n",
    "print(out_test)\n",
    "out_test.to_csv('results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1aa56d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ibab/Desktop/ml_proj\r\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# cols = np.vsplit(predictions, 3)\n",
    "# print(cols)\n",
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
